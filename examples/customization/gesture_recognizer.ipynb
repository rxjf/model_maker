{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-9BpIlqAZci"
      },
      "source": [
        "Project: /mediapipe/_project.yaml\n",
        "Book: /mediapipe/_book.yaml\n",
        "\n",
        "<link rel=\"stylesheet\" href=\"/mediapipe/site.css\">\n",
        "\n",
        "# Hand gesture recognition model customization guide\n",
        "\n",
        "<table align=\"left\" class=\"buttons\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/customization/gesture_recognizer.ipynb\" target=\"_blank\">\n",
        "      <img src=\"https://developers.google.com/static/mediapipe/solutions/customization/colab-logo-32px_1920.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://github.com/googlesamples/mediapipe/blob/main/examples/customization/gesture_recognizer.ipynb\" target=\"_blank\">\n",
        "      <img src=\"https://developers.google.com/static/mediapipe/solutions/customization/github-logo-32px_1920.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JO1GUwC1_T2x"
      },
      "outputs": [],
      "source": [
        "#@title License information\n",
        "# Copyright 2023 The MediaPipe Authors.\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "#\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFBcmjzf0JLE"
      },
      "source": [
        "The MediaPipe Model Maker package is a low-code solution for customizing on-device machine learning (ML) Models.\n",
        "\n",
        "This notebook shows the end-to-end process of customizing a gesture recognizer model for recognizing some common hand gestures in the [HaGRID](https://www.kaggle.com/datasets/innominate817/hagrid-sample-30k-384p) dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGM0PT490LiR"
      },
      "source": [
        "## Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVVxZNfo0M0y"
      },
      "source": [
        "Install the MediaPipe Model Maker package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DBLRE-fqlO5",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install mediapipe-model-maker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3CvTNmB1WiY"
      },
      "source": [
        "Import the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c74UL9oI0VKU"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "from mediapipe_model_maker import gesture_recognizer\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "c_M4sgEzANv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IppoENBmAuFn"
      },
      "source": [
        "## Simple End-to-End Example\n",
        "\n",
        "This end-to-end example uses Model Maker to customize a model for on-device gesture recognition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8fMLXTdD6tW"
      },
      "source": [
        "### Get the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TwDFilngzjs"
      },
      "source": [
        "The dataset for gesture recognition in model maker requires the following format: `<dataset_path>/<label_name>/<img_name>.*`. In addition, one of the label names (`label_names`) must be `none`. The `none` label represents any gesture that isn't classified as one of the other gestures.\n",
        "\n",
        "This example uses a rock paper scissors dataset sample which is downloaded from GCS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dwmyg5MnR_y"
      },
      "outputs": [],
      "source": [
        "dataset_path = \"drive/MyDrive/ColabWS/GestureCustomize/gesture2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiWb9Tu3lBBI"
      },
      "source": [
        "Verify the rock paper scissors dataset by printing the labels. There should be 4 gesture labels, with one of them being the `none` gesture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgadM4VDj3Y2"
      },
      "outputs": [],
      "source": [
        "print(dataset_path)\n",
        "labels = []\n",
        "for i in os.listdir(dataset_path):\n",
        "  if os.path.isdir(os.path.join(dataset_path, i)):\n",
        "    labels.append(i)\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CA0o59OMjqmV"
      },
      "source": [
        "To better understand the dataset, plot a couple of example images for each gesture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx8PsrwYjvgO",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from google.colab.patches import cv2_imshow,cv2\n",
        "\n",
        "NUM_EXAMPLES = 1\n",
        "\n",
        "for label in labels:\n",
        "  label_dir = os.path.join(dataset_path, label)\n",
        "  example_filenames = os.listdir(label_dir)[:NUM_EXAMPLES]\n",
        "  for i in range(NUM_EXAMPLES):\n",
        "    img=cv2.imread(os.path.join(label_dir, example_filenames[i]))\n",
        "    cv2_imshow(img)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWXwEXSXlg7d"
      },
      "source": [
        "### Run the example\n",
        "The workflow consists of 4 steps which have been separated into their own code blocks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF9ArLQXIu25"
      },
      "source": [
        "**Load the dataset**\n",
        "\n",
        "Load the dataset located at `dataset_path` by using the `Dataset.from_folder` method. When loading the dataset, run the pre-packaged hand detection model from MediaPipe Hands to detect the hand landmarks from the images. Any images without detected hands are ommitted from the dataset. The resulting dataset will contain the extracted hand landmark positions from each image, rather than images themselves.\n",
        "\n",
        "The `HandDataPreprocessingParams` class contains two configurable options for the data loading process:\n",
        "* `shuffle`: A boolean controlling whether to shuffle the dataset. Defaults to true.\n",
        "* `min_detection_confidence`: A float between 0 and 1 controlling the confidence threshold for hand detection.\n",
        "\n",
        "Split the dataset: 80% for training, 10% for validation, and 10% for testing."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2022 The MediaPipe Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"Gesture recognition dataset library.\"\"\"\n",
        "\n",
        "import dataclasses\n",
        "import os\n",
        "import random\n",
        "from typing import List, Optional\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from mediapipe_model_maker.python.core.data import classification_dataset\n",
        "from mediapipe_model_maker.python.core.utils import model_util\n",
        "from mediapipe_model_maker.python.vision.gesture_recognizer import constants\n",
        "from mediapipe_model_maker.python.vision.gesture_recognizer import metadata_writer\n",
        "from mediapipe.python._framework_bindings import image as image_module\n",
        "from mediapipe.tasks.python.core import base_options as base_options_module\n",
        "from mediapipe.tasks.python.vision import hand_landmarker as hand_landmarker_module\n",
        "\n",
        "_Image = image_module.Image\n",
        "_HandLandmarker = hand_landmarker_module.HandLandmarker\n",
        "_HandLandmarkerOptions = hand_landmarker_module.HandLandmarkerOptions\n",
        "_HandLandmarkerResult = hand_landmarker_module.HandLandmarkerResult\n",
        "\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class HandDataPreprocessingParams:\n",
        "  \"\"\"A dataclass wraps the hand data preprocessing hyperparameters.\n",
        "\n",
        "  Attributes:\n",
        "    shuffle: A boolean controlling if shuffle the dataset. Default to true.\n",
        "    min_detection_confidence: confidence threshold for hand detection.\n",
        "  \"\"\"\n",
        "  shuffle: bool = True\n",
        "  min_detection_confidence: float = 0.7\n",
        "\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class HandData:\n",
        "  \"\"\"A dataclass represents hand data for training gesture recognizer model.\n",
        "\n",
        "  See https://google.github.io/mediapipe/solutions/hands#mediapipe-hands for\n",
        "  more details of the hand gesture data API.\n",
        "\n",
        "  Attributes:\n",
        "    hand: normalized hand landmarks of shape 21x3 from the screen based\n",
        "      hand-landmark model.\n",
        "    world_hand: hand landmarks of shape 21x3 in world coordinates.\n",
        "    handedness: Collection of handedness confidence of the detected hands (i.e.\n",
        "      is it a left or right hand).\n",
        "  \"\"\"\n",
        "  hand: List[List[float]]\n",
        "  world_hand: List[List[float]]\n",
        "  handedness: List[float]\n",
        "\n",
        "\n",
        "def _validate_data_sample(data: _HandLandmarkerResult) -> bool:\n",
        "  \"\"\"Validates the input hand data sample.\n",
        "\n",
        "  Args:\n",
        "    data: input hand data sample.\n",
        "\n",
        "  Returns:\n",
        "    False if the input data namedtuple does not contain the fields including\n",
        "    'multi_hand_landmarks' or 'multi_hand_world_landmarks' or 'multi_handedness'\n",
        "    or any of these attributes' values are none. Otherwise, True.\n",
        "  \"\"\"\n",
        "  if data.hand_landmarks is None or not data.hand_landmarks:\n",
        "    return False\n",
        "  if data.hand_world_landmarks is None or not data.hand_world_landmarks:\n",
        "    return False\n",
        "  if data.handedness is None or not data.handedness:\n",
        "    return False\n",
        "  return True\n",
        "\n",
        "\n",
        "def _get_hand_data(all_image_paths: List[str],\n",
        "                   min_detection_confidence: float) -> List[Optional[HandData]]:\n",
        "  \"\"\"Computes hand data (landmarks and handedness) in the input image.\n",
        "\n",
        "  Args:\n",
        "    all_image_paths: all input image paths.\n",
        "    min_detection_confidence: hand detection confidence threshold\n",
        "\n",
        "  Returns:\n",
        "    A HandData object. Returns None if no hand is detected.\n",
        "  \"\"\"\n",
        "  hand_data_result = []\n",
        "  hand_detector_model_buffer = model_util.load_tflite_model_buffer(\n",
        "      constants.HAND_DETECTOR_TFLITE_MODEL_FILE.get_path()\n",
        "  )\n",
        "  hand_landmarks_detector_model_buffer = model_util.load_tflite_model_buffer(\n",
        "      constants.HAND_LANDMARKS_DETECTOR_TFLITE_MODEL_FILE.get_path()\n",
        "  )\n",
        "  hand_landmarker_writer = metadata_writer.HandLandmarkerMetadataWriter(\n",
        "      hand_detector_model_buffer, hand_landmarks_detector_model_buffer)\n",
        "  hand_landmarker_options = _HandLandmarkerOptions(\n",
        "      base_options=base_options_module.BaseOptions(\n",
        "          model_asset_buffer=hand_landmarker_writer.populate()),\n",
        "      num_hands=1,\n",
        "      min_hand_detection_confidence=min_detection_confidence,\n",
        "      min_hand_presence_confidence=0.5,\n",
        "      min_tracking_confidence=1,\n",
        "  )\n",
        "  with _HandLandmarker.create_from_options(\n",
        "      hand_landmarker_options) as hand_landmarker:\n",
        "    for index, path in enumerate(all_image_paths):\n",
        "      tf.compat.v1.logging.info('Loading image %s', path)\n",
        "      image = _Image.create_from_file(path)\n",
        "      data = hand_landmarker.detect(image)\n",
        "\n",
        "      img=cv2.imread(path)\n",
        "      print(path, index)\n",
        "      if not _validate_data_sample(data):\n",
        "        cv2_imshow(img)\n",
        "        hand_data_result.append(None)\n",
        "        continue\n",
        "      hand_landmarks = [[hand_landmark.x, hand_landmark.y, hand_landmark.z]\n",
        "                        for hand_landmark in data.hand_landmarks[0]]\n",
        "      hand_world_landmarks = [[\n",
        "          hand_landmark.x, hand_landmark.y, hand_landmark.z\n",
        "      ] for hand_landmark in data.hand_world_landmarks[0]]\n",
        "      handedness_scores = [\n",
        "          handedness.score for handedness in data.handedness[0]\n",
        "      ]\n",
        "      hand_data_result.append(\n",
        "          HandData(\n",
        "              hand=hand_landmarks,\n",
        "              world_hand=hand_world_landmarks,\n",
        "              handedness=handedness_scores))\n",
        "\n",
        "      height, width, _ = img.shape\n",
        "      for i, landmark in enumerate(hand_landmarks):\n",
        "        x = int(landmark[0] * width)\n",
        "        y = int(landmark[1] * height)\n",
        "        if i % 2 == 0:  # 偶数索引位置的点\n",
        "          color = (0, 0, 0)  # 黑色\n",
        "        else:\n",
        "          color = (0, 255, 0)  # 绿色\n",
        "        cv2.circle(img, (x, y), 5, color, -1)\n",
        "\n",
        "      cv2.line(img, (int(hand_landmarks[0][0]*width), int(hand_landmarks[0][1]*height)), (int(hand_landmarks[1][0]*width), int(hand_landmarks[1][1]*height)),\n",
        "\t(0, 0, 255), 2);\n",
        "      cv2.line(img, (int(hand_landmarks[1][0]*width), int(hand_landmarks[1][1]*height)), (int(hand_landmarks[2][0]*width), int(hand_landmarks[2][1]*height)),\n",
        "\t(0, 0, 255), 2);\n",
        "      cv2.line(img, (int(hand_landmarks[2][0]*width), int(hand_landmarks[2][1]*height)), (int(hand_landmarks[3][0]*width), int(hand_landmarks[3][1]*height)),\n",
        "\t(0, 0, 255), 2);\n",
        "      cv2.line(img, (int(hand_landmarks[3][0]*width), int(hand_landmarks[3][1]*height)), (int(hand_landmarks[4][0]*width), int(hand_landmarks[4][1]*height)),\n",
        "\t(0, 0, 255), 2);\n",
        "      cv2.line(img, (int(hand_landmarks[0][0]*width), int(hand_landmarks[0][1]*height)), (int(hand_landmarks[5][0]*width), int(hand_landmarks[5][1]*height)),\n",
        "\t(0, 255, 255), 2);\n",
        "      cv2.line(img, (int(hand_landmarks[5][0]*width), int(hand_landmarks[5][1]*height)), (int(hand_landmarks[6][0]*width), int(hand_landmarks[6][1]*height)),\n",
        "\t(0, 255, 255), 2);\n",
        "      cv2.line(img, (int(hand_landmarks[6][0]*width), int(hand_landmarks[6][1]*height)), (int(hand_landmarks[7][0]*width), int(hand_landmarks[7][1]*height)),\n",
        "\t(0, 255, 255), 2);\n",
        "      cv2.line(img, (int(hand_landmarks[7][0]*width), int(hand_landmarks[7][1]*height)), (int(hand_landmarks[8][0]*width), int(hand_landmarks[8][1]*height)),\n",
        "\t(0, 255, 255), 2);\n",
        "      cv2.line(img, (int(hand_landmarks[0][0]*width), int(hand_landmarks[0][1]*height)), (int(hand_landmarks[9][0]*width), int(hand_landmarks[9][1]*height)),\n",
        "\t(255, 0, 255), 2);\n",
        "      cv2.line(img, (int(hand_landmarks[9][0]*width), int(hand_landmarks[9][1]*height)), (int(hand_landmarks[10][0]*width), int(hand_landmarks[10][1]*height)),\n",
        "\t(255, 0, 255), 2);\n",
        "      cv2.line(img, (int(hand_landmarks[10][0]*width), int(hand_landmarks[10][1]*height)), (int(hand_landmarks[11][0]*width), int(hand_landmarks[11][1]*height)),\n",
        "\t(255, 0, 255), 2);\n",
        "      cv2.line(img, (int(hand_landmarks[11][0]*width), int(hand_landmarks[11][1]*height)), (int(hand_landmarks[12][0]*width), int(hand_landmarks[12][1]*height)),\n",
        "\t(255, 0, 255), 2);\n",
        "      cv2.line(img, (int(hand_landmarks[0][0]*width), int(hand_landmarks[0][1]*height)), (int(hand_landmarks[13][0]*width), int(hand_landmarks[13][1]*height)),\n",
        "\t(255, 0, 0), 2);\n",
        "      cv2.line(img, (int(hand_landmarks[13][0]*width), int(hand_landmarks[13][1]*height)), (int(hand_landmarks[14][0]*width), int(hand_landmarks[14][1]*height)),\n",
        "\t(255, 0, 0), 2);\n",
        "      cv2.line(img, (int(hand_landmarks[14][0]*width), int(hand_landmarks[14][1]*height)), (int(hand_landmarks[15][0]*width), int(hand_landmarks[15][1]*height)),\n",
        "\t(255, 0, 0), 2);\n",
        "      cv2.line(img, (int(hand_landmarks[15][0]*width), int(hand_landmarks[15][1]*height)), (int(hand_landmarks[16][0]*width), int(hand_landmarks[16][1]*height)),\n",
        "\t(255, 0, 0), 2);\n",
        "      cv2.line(img, (int(hand_landmarks[0][0]*width), int(hand_landmarks[0][1]*height)), (int(hand_landmarks[17][0]*width), int(hand_landmarks[17][1]*height)),\n",
        "\t(255, 255, 255), 2);\n",
        "      cv2.line(img, (int(hand_landmarks[17][0]*width), int(hand_landmarks[17][1]*height)), (int(hand_landmarks[18][0]*width), int(hand_landmarks[18][1]*height)),\n",
        "\t(255, 255, 255), 2);\n",
        "      cv2.line(img, (int(hand_landmarks[18][0]*width), int(hand_landmarks[18][1]*height)), (int(hand_landmarks[19][0]*width), int(hand_landmarks[19][1]*height)),\n",
        "\t(255, 255, 255), 2);\n",
        "      cv2.line(img, (int(hand_landmarks[19][0]*width), int(hand_landmarks[19][1]*height)), (int(hand_landmarks[20][0]*width), int(hand_landmarks[20][1]*height)),\n",
        "\t(255, 255, 255), 2);\n",
        "      cv2_imshow(img)\n",
        "  return hand_data_result\n",
        "\n",
        "class DatasetII(classification_dataset.ClassificationDataset):\n",
        "  \"\"\"Dataset library for hand gesture recognizer.\"\"\"\n",
        "\n",
        "  @classmethod\n",
        "  def from_folder_base(\n",
        "      cls,\n",
        "      dirname: str,\n",
        "      hparams: Optional[HandDataPreprocessingParams] = None\n",
        "  ) -> List[Optional[HandData]]:\n",
        "    data_root = os.path.abspath(dirname)\n",
        "\n",
        "    # Assumes the image data of the same label are in the same subdirectory,\n",
        "    # gets image path and label names.\n",
        "    all_image_paths = list(tf.io.gfile.glob(data_root + r'/*/*'))\n",
        "    if not all_image_paths:\n",
        "      raise ValueError('Image dataset directory is empty.')\n",
        "\n",
        "    if not hparams:\n",
        "      hparams = HandDataPreprocessingParams()\n",
        "\n",
        "    if hparams.shuffle:\n",
        "      # Random shuffle data.\n",
        "      random.shuffle(all_image_paths)\n",
        "\n",
        "    return _get_hand_data(\n",
        "        all_image_paths=all_image_paths,\n",
        "        min_detection_confidence=hparams.min_detection_confidence)\n",
        "\n",
        "  @classmethod\n",
        "  def from_folder(\n",
        "      cls,\n",
        "      dirname: str,\n",
        "      hand_data: List[Optional[HandData]]\n",
        "  ) -> classification_dataset.ClassificationDataset:\n",
        "    \"\"\"Loads images and labels from the given directory.\n",
        "\n",
        "    Directory contents are expected to be in the format:\n",
        "    <root_dir>/<gesture_name>/*.jpg\". One of the `gesture_name` must be `none`\n",
        "    (case insensitive). The `none` sub-directory is expected to contain images\n",
        "    of hands that don't belong to other gesture classes in <root_dir>. Assumes\n",
        "    the image data of the same label are in the same subdirectory.\n",
        "\n",
        "    Args:\n",
        "      dirname: Name of the directory containing the data files.\n",
        "      hparams: Optional hyperparameters for processing input hand gesture\n",
        "        images.\n",
        "\n",
        "    Returns:\n",
        "      Dataset containing landmarks, labels, and other related info.\n",
        "\n",
        "    Raises:\n",
        "      ValueError: if the input data directory is empty or the label set does not\n",
        "        contain label 'none' (case insensitive).\n",
        "    \"\"\"\n",
        "    data_root = os.path.abspath(dirname)\n",
        "\n",
        "    # Assumes the image data of the same label are in the same subdirectory,\n",
        "    # gets image path and label names.\n",
        "    all_image_paths = list(tf.io.gfile.glob(data_root + r'/*/*'))\n",
        "    if not all_image_paths:\n",
        "      raise ValueError('Image dataset directory is empty.')\n",
        "\n",
        "    label_names = sorted(\n",
        "        name for name in os.listdir(data_root)\n",
        "        if os.path.isdir(os.path.join(data_root, name)))\n",
        "    if 'none' not in [v.lower() for v in label_names]:\n",
        "      raise ValueError('Label set does not contain label \"None\".')\n",
        "    # Move label 'none' to the front of label list.\n",
        "    none_idx = [v.lower() for v in label_names].index('none')\n",
        "    none_value = label_names.pop(none_idx)\n",
        "    label_names.insert(0, none_value)\n",
        "\n",
        "    index_by_label = dict(\n",
        "        (name, index) for index, name in enumerate(label_names))\n",
        "    all_gesture_indices = [\n",
        "        index_by_label[os.path.basename(os.path.dirname(path))]\n",
        "        for path in all_image_paths\n",
        "    ]\n",
        "\n",
        "    # Get a list of the valid hand landmark sample in the hand data list.\n",
        "    valid_indices = [\n",
        "        i for i in range(len(hand_data)) if hand_data[i] is not None\n",
        "    ]\n",
        "    # Remove 'None' element from the hand data and label list.\n",
        "    valid_hand_data = [dataclasses.asdict(hand_data[i]) for i in valid_indices]\n",
        "    if not valid_hand_data:\n",
        "      raise ValueError('No valid hand is detected.')\n",
        "\n",
        "    valid_label = [all_gesture_indices[i] for i in valid_indices]\n",
        "\n",
        "    # Convert list of dictionaries to dictionary of lists.\n",
        "    hand_data_dict = {\n",
        "        k: [lm[k] for lm in valid_hand_data] for k in valid_hand_data[0]\n",
        "    }\n",
        "    hand_ds = tf.data.Dataset.from_tensor_slices(hand_data_dict)\n",
        "\n",
        "    embedder_model = model_util.load_keras_model(\n",
        "        constants.GESTURE_EMBEDDER_KERAS_MODEL_FILES.get_path()\n",
        "    )\n",
        "\n",
        "    hand_ds = hand_ds.batch(batch_size=1)\n",
        "    hand_embedding_ds = hand_ds.map(\n",
        "        map_func=lambda feature: embedder_model(dict(feature)),\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    hand_embedding_ds = hand_embedding_ds.unbatch()\n",
        "\n",
        "    # Create label dataset\n",
        "    label_ds = tf.data.Dataset.from_tensor_slices(\n",
        "        tf.cast(valid_label, tf.int64))\n",
        "\n",
        "    label_one_hot_ds = label_ds.map(\n",
        "        map_func=lambda index: tf.one_hot(index, len(label_names)),\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    # Create a dataset with (hand_embedding, one_hot_label) pairs\n",
        "    hand_embedding_label_ds = tf.data.Dataset.zip(\n",
        "        (hand_embedding_ds, label_one_hot_ds))\n",
        "\n",
        "    tf.compat.v1.logging.info(\n",
        "        'Load valid hands with size: {}, num_label: {}, labels: {}.'.format(\n",
        "            len(valid_hand_data), len(label_names), ','.join(label_names)))\n",
        "    return DatasetII(\n",
        "        dataset=hand_embedding_label_ds,\n",
        "        label_names=label_names,\n",
        "        size=len(valid_hand_data),\n",
        "    )"
      ],
      "metadata": {
        "id": "zSX-NK-fDNlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hand_data_res = DatasetII.from_folder_base(dirname=dataset_path,\n",
        "    hparams=gesture_recognizer.HandDataPreprocessingParams())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GWb0jTuRPZzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_indices = [\n",
        "        i for i in range(len(hand_data_res)) if hand_data_res[i] is not None\n",
        "    ]\n",
        "\n",
        "print(valid_indices)"
      ],
      "metadata": {
        "id": "ysAAL88ceuuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b = [1, 3, 5] # set this\n",
        "for index in b:\n",
        "  if 0 <= index < len(hand_data_res):\n",
        "    hand_data_res[index] = None"
      ],
      "metadata": {
        "id": "w7-VsK-RfQXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_indices = [\n",
        "        i for i in range(len(hand_data_res)) if hand_data_res[i] is not None\n",
        "    ]\n",
        "\n",
        "print(valid_indices)"
      ],
      "metadata": {
        "id": "pDX3H58ygSOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = DatasetII.from_folder(dirname=dataset_path,\n",
        "    hand_data=hand_data_res)"
      ],
      "metadata": {
        "id": "XuUNcNSlPZlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTTNZsolKXiT"
      },
      "outputs": [],
      "source": [
        "# data = gesture_recognizer.Dataset.from_folder(\n",
        "#     dirname=dataset_path,\n",
        "#     hparams=gesture_recognizer.HandDataPreprocessingParams()\n",
        "# )\n",
        "train_data, rest_data = data.split(0.8)\n",
        "validation_data, test_data = rest_data.split(0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndTh_ZyEIeKV"
      },
      "source": [
        "**Train the model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAXWc3bv8hpe"
      },
      "source": [
        "Train the custom gesture recognizer by using the create method and passing in the training data, validation data, model options, and hyperparameters. For more information on model options and hyperparameters, see the [Hyperparameters](#hyperparameters) section below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yk0UiRB6NZrb"
      },
      "outputs": [],
      "source": [
        "hparams = gesture_recognizer.HParams(export_dir=\"exported_model\")\n",
        "options = gesture_recognizer.GestureRecognizerOptions(hparams=hparams)\n",
        "model = gesture_recognizer.GestureRecognizer.create(\n",
        "    train_data=train_data,\n",
        "    validation_data=validation_data,\n",
        "    options=options\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nED7mdIO9YS6"
      },
      "source": [
        "**Evaluate the model performance**\n",
        "\n",
        "After training the model, evaluate it on a test dataset and print the loss and accuracy metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdOqllqx9YKy"
      },
      "outputs": [],
      "source": [
        "loss, acc = model.evaluate(test_data, batch_size=1)\n",
        "print(f\"Test loss:{loss}, Test accuracy:{acc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJLramjy9gvy"
      },
      "source": [
        "**Export to Tensorflow Lite Model**\n",
        "\n",
        "After creating the model, convert and export it to a Tensorflow Lite model format for later use on an on-device application. The export also includes model metadata, which includes the label file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmNaFXytijVg"
      },
      "outputs": [],
      "source": [
        "model.export_model()\n",
        "!ls exported_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yfN_47qjjOC"
      },
      "outputs": [],
      "source": [
        "files.download('exported_model/gesture_recognizer.task')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulqyNGmTCKeU"
      },
      "source": [
        "## Run the model on-device\n",
        "\n",
        "To use the TFLite model for on-device usage through MediaPipe Tasks, refer to the Gesture Recognizer [overview page](https://developers.google.com/mediapipe/solutions/vision/gesture_recognizer)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1tiLGGRcvhy"
      },
      "source": [
        "## Hyperparameters {:#hyperparameters}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1UMEG85hQL_"
      },
      "source": [
        "You can further customize the model using the `GestureRecognizerOptions` class, which has two optional parameters for `ModelOptions` and `HParams`. Use the `ModelOptions` class to customize parameters related to the model itself, and the `HParams` class to customize other parameters related to training and saving the model.\n",
        "\n",
        "`ModelOptions` has one customizable parameter that affects accuracy:\n",
        "* `dropout_rate`: The fraction of the input units to drop. Used in dropout layer. Defaults to 0.05.\n",
        "* `layer_widths`: A list of hidden layer widths for the gesture model. Each element in the list will create a new hidden layer with the specified width. The hidden layers are separated with BatchNorm, Dropout, and ReLU. Defaults to an empty list(no hidden layers).\n",
        "\n",
        "`HParams` has the following list of customizable parameters which affect model accuracy:\n",
        "* `learning_rate`: The learning rate to use for gradient descent training. Defaults to 0.001.\n",
        "* `batch_size`: Batch size for training. Defaults to 2.\n",
        "* `epochs`: Number of training iterations over the dataset. Defaults to 10.\n",
        "* `steps_per_epoch`: An optional integer that indicates the number of training steps per epoch. If not set, the training pipeline calculates the default steps per epoch as the training dataset size divided by batch size.\n",
        "* `shuffle`: True if the dataset is shuffled before training. Defaults to False.\n",
        "* `lr_decay`: Learning rate decay to use for gradient descent training. Defaults to 0.99.\n",
        "* `gamma`: Gamma parameter for focal loss. Defaults to 2\n",
        "\n",
        "Additional `HParams` parameter that does not affect model accuracy:\n",
        "* `export_dir`: The location of the model checkpoint files and exported model files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psvVZeSYBLfV"
      },
      "source": [
        "For example, the following trains a new model with the dropout_rate of 0.2 and learning rate of 0.003."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxMOI8o6iNLu"
      },
      "outputs": [],
      "source": [
        "hparams = gesture_recognizer.HParams(learning_rate=0.003, export_dir=\"exported_model_2\")\n",
        "model_options = gesture_recognizer.ModelOptions(dropout_rate=0.2)\n",
        "options = gesture_recognizer.GestureRecognizerOptions(model_options=model_options, hparams=hparams)\n",
        "model_2 = gesture_recognizer.GestureRecognizer.create(\n",
        "    train_data=train_data,\n",
        "    validation_data=validation_data,\n",
        "    options=options\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cekuTJiBbv9"
      },
      "source": [
        "Evaluate the newly trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRH96bm-BbAo"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model_2.evaluate(test_data)\n",
        "print(f\"Test loss:{loss}, Test accuracy:{accuracy}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}